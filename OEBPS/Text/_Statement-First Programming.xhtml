<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>Statement-First Programming</title>
  <link href="../Styles/Global.css" rel="stylesheet" type="text/css" />
</head>

<body>
  <h1 id="sigil_toc_id_9">Statement-First programming</h1>

  <h2>What's The Point of Writing Specification After The Fact?</h2>

  <p>In the last chapter, I said that in TDD a 'test' takes another role - one of a statement being a part of a specification. If we put things this way, then the whole controversial concept of "writing a test before the code" does not pose a problem at all. Quite the contrary - it only seems natural to specify what we're going to write before we attempt to write it. Does the other way round even make sense? A specification written after completing the implementation is nothing more than an attempt at documenting the existing solution. Sure, such attempts can provide some value when done as a kind of reverse-engineering (i.e. writing specification for something that was implemented long ago and we don't really know the exact business rules or policies, which we discover as we document the existing solution) - it has an excitement of discovery in it, but doing it just after we, ourselves, made all the decisions seems like a waste of time, not to mention that it's dead boring (Don't believe me? Try to implement a simple calculator app and the write specification for it just after it's implemented and working). Anyway, specifying how something should work after the fact can hardly be considered creative.</p>

  <p>Oh, and did I tell you that without a specification of any kind we don't really know whether we're done implementing our changes or not (because in order to know it, we need to compare the implemented functionality to 'something', even if this 'something' is only in the customer's head).</p>

  <p>Another thing I mentioned in the previous chapter was that one of the differences between a textual specification and our Specification consisting of executable Statements is that, although the code follows the Specification, we don't write our Specification fully up-front. The usual sequence is to specify a bit first and then code a bit, then repeat one Statement at a time. When doing TDD, we're traversing repeatedly through few phases that make up a cycle. We like these cycles to be short, so that we can get a quick feedback. Being able to get this quick feedback is essential, because it allows us to move forward with confidence that what we already have works as we intended. Also, it allows us to use the knowledge we gained in the previous cycle to make the next cycle more efficient (if you don't believe me that quick feedback is good, ask yourself a question: "how many times a day do I compile the code I work on?").</p>

  <p>Reading so much about cycles, it's probably no surprise to you that the traditional illustration of the TDD process is modeled visually as a circle-like flow:</p>

  <p><img alt="Traditional_TDD_cycle" src="../Images/Traditional_TDD_cycle.png" /></p>

  <p>Note that the above form uses the traditional terminology of TDD, so before I explain the steps, I'll translate it to use our terms of Specification and Statements:</p>

  <p><img alt="Modern_TDD_cycle" src="../Images/Modern_TDD_cycle.png" /></p>

  <p>The second version seems more like common sense than the first one - specifying how something should behave before putting that behavior in place is way more intuitive than testing something that does not exist.</p>

  <p>Anyway, these three steps demand some explanation. In the coming chapters, I'll give you some examples of how this process works in practice and introduce an expanded version, but in the meantime, its sufficient to say that:</p>

  <dl>
    <dt>Write an unfulfilled Statement</dt>

    <dd>means that the Statement evaluates to false (it shows on the test list as unfulfilled - in most xUnit frameworks, it will be marked with red color)</dd>

    <dt>Fulfill it</dt>

    <dd>means that we write just enough code to fulfill the Statement (in most xUnit frameworks, the fulfilled Statement will be marked with green color). Later in the course of the book, you'll see how small can be "just enough"</dd>

    <dt>Refactor</dt>

    <dd>is a step that I have silently discarded so far (and will do so for at least few next chapters. Don't worry, we'll get back to it eventually). Basically, it boils down to using the safety net of executable specification we already have in place to safely enhance the quality of the covered code while all mistakes we make in the process are quickly discovered by the running Specification.</dd>
  </dl>

  <p>By the way, this process is sometimes referred to as "Red-Green-Refactor". I'm just mentioning it here for the record - I'm not planning to use this term further in the book.</p>

  <h2>The Benefit of Failure</h2>

  <p>Look again at the drawing with TDD process - can you spot a single word I underlined? Yes, it's <strong>unfulfilled</strong>. It means that when you write a Statement, you have to evaluate it (i.e. run it) and watch it fail its assertions before providing implementation that makes this Statement true. Why is that so important? Isn't it just enough to write the Statement first? Why run it and watch it fail?</p>

  <p>There are multiple reasons and I'll try to outline few of them briefly.</p>

  <h3>You don't know whether the Statement can ever be false until you see it evaluate to false</h3>

  <p>Every accurate Statement (do I have to tell you that such Statements are what we're interested in?) fails when it isn't fulfilled and passes when it is. That's one of the main reasons we write it - to receive this feedback. Also, after being fulfilled, the Statement becomes a part of the executable specification and starts failing as soon as the code stops fulfilling it (e.g. as a result of mistake made during code rework). When your run a Statement after it's implemented and it is evaluated as true, how do you know whether it really describes a need accurately? You didn't ever watch it fail, so how do you know it ever will?</p>

  <p>The first time I encountered this argument (it was before I started thinking of unit tests as executable specification), it quickly raised my self-defense mechanism: "seriously?" - I thought - "I'm a wise person, I know what I'm writing. If I make my unit tests small enough, it's self-evident that I'm describing the correct behavior. This is paranoid". However, life quickly verified my claims and I was forced to withdraw my arguments. Let me describe, from my experience, three ways (there are more, I just forgot the rest :-D) one can really put in a Statement that is always evaluated as true, regardless of the code being correct or not (i.e. a Statement that cheats you into thinking it's fulfilled even when it's not):</p>

  <h4>1. Accidental omission of adding a Statement to Specification.</h4>

  <p>However funny this may sound, it happened to me few times. The example I'm going to give is from C#, but almost every xUnit framework in almost every language has some kind of mechanism of marking methods as Statements, whether by attributes (C#, e.g. xUnit.Net's <code>Fact</code> attribute) or annotations (Java) or with macros (C and C++) or by inheriting from common class, or just a naming convention.</p>

  <p>Let's take xUnit.Net as an example. As I stated previously, In xUnit.Net, to turn a method into a Statement, you mark it with <code>[Fact]</code> attribute the following way:</p>
  <pre class="brush:csharp;">public class CalculatorSpecification
{
  [Fact]
  public void ShouldDisplayAdditionResultAsSumOfArguments() 
  {
    //... 
  }
}
</pre>

  <p>Now, imagine that you're writing this Statement post-factum as a unit test in an environment that has, let's say, more than thirty Statements - you've written the code, now you're just creating a test after test "to ensure" (as you see, this is not my favorite reason for writing unit tests) the code works. Code, test - pass, test - pass, test - pass. You almost always evaluate your code against the whole Specification, since it's usually easier than selectig what to evaluate each time, plus, you get more confidence this way that you didn't break by mistake something that's already working. So, this is really: Code, Test - all pass, test - all pass, test - all pass... Hopefully, you use some kind of snippets mechanism for creating new Statements, but if not (and many don't actually do this), once in a while, you do something like this:</p>
  <pre class="brush:csharp;">
public class CalculatorSpecification
{
  //... some Statements here

  //oops... forgot to copy-paste the attribute!
  public void ShouldDisplayZeroWhenResetIsPerformed()
  {
    //... 
  }
}
</pre>

  <p>And you don't even notice that this will not be evaluated with the rest of the Specification, because it already consists of so many Statements that it's almost irrational to search for your added Statement in the list and make sure it's there each time. Also, note that the fact that you omitted the addition, does not disturb your work flow: test - all pass, test - all pass, test - all pass... In other words, your process does not give you any feedback on your mistake. So, what you end up is a Statement that not only will never be false - <strong>it will never be evaluated</strong>.</p>

  <p>How does treating tests as Statements and evaluating them before making them true help here? <strong>Because then, a Statement that starts off being evaluated as true is what DOES disturb your work flow.</strong> In TDD, the work flow is: Statement - unfulfilled - fulfilled (ok, and refactor, but for the sake of THIS discussion, it doesn't matter so much), Statement - unfulfilled - fulfilled, Statement - unfulfilled - fulfilled... So every time you fail to see the "unfulfilled" stage, you get feedback from your process that something suspicious is happening. This lets you investigate and, if necessary, fix the situation at hand.</p>

  <h4>2. Misplacing mock setup</h4>

  <p>Ok, this may sound even funnier (well, honestly, most mistakes sound funny), but it also happened to me a couple of times, so it makes sense to mention it. The example I'm going to show uses manual mocks, but this can happen with dynamic mocks as well, especially if you're in a hurry.</p>

  <p>Let's take a look at the following Statement saying that setting a value higher than allowed to a field of a frame should produce error result:</p>
  <pre class="brush:csharp;">[Fact]
public void ShouldRecognizeTimeSlotAboveMaximumAllowedAsInvalid()
{
  //GIVEN
  var frame = new FrameMock(); //manual mock
  var validation = new Validation();
  var timeSlotAboveMaximumAllowed = TimeSlot.MaxAllowed + 1;

  //WHEN
  var result = validation.PerformForTimeSlotIn(frame);
  frame.GetTimeSlot_Returns 
    = timeSlotAboveMaximumAllowed;

  //THEN
  Assert.False(result.Passed);
  Assert.Equal(
    ValidationFailureReasons.AboveAcceptableLimit, 
    result.Reason);
}
</pre>

  <p>Note how the method <code>PerformForTimeSlotIn()</code>, which triggers the specified behavior is accidentally called BEFORE the mock is actually set up and the set up return value is never taken into account. By some strange coincidence, this error did not alter the expected end result so we didn't even notice. It sometimes turns out like this, most often in case of various boundary values (nulls etc.).</p>

  <h4>3. Using static data inside production code</h4>

  <p>Once in a while, you have to jump in and add some new Statements to some class Specification and some logic to the class itself. Let's assume that the class and its existing specification was written by someone else. Imagine this code is a wrapper around your product XML configuration file. You decide to write your Statements AFTER applying the changes ("well", you can say, "I'm all protected by the Specification that's already in place, so I can make my change without risking regression, then just test my changes and it's all good...").</p>

  <p>So, you start writing the new Statement. The Specification class already contains a field member like this:</p>
  <pre class="brush:csharp;">public class XmlConfigurationSpecification
{
  XmlConfiguration config = new XmlConfiguration(xmlFixtureString);
  
  //...
  //...

</pre>

  <p>What it does is to set up an object used by every Statement. So, each Statement uses a <code>config</code> object initialized with the same <code>xmlConfiguration</code> string value. The string is already pretty huge and messy, since it was made to contain what's required by all existing Statements. You need to write tests for is a little corner case that does not need all this crap that's inside this string. So, you decide to start fresh and create a separate object of <code>XmlConfiguration</code> class with your own, minimal string. Your Statement begins like this:</p>
  <pre class="brush:csharp;">string customFixture = CreateMyOwnFixtureForThisTestOnly();
var configuration = new XmlConfiguration(customFixture);
...
</pre>

  <p>And it passes - cool... not. Ok, what's wrong with this? Nothing big, unless you read the source code of XmlConfiguration class carefully. Inside, you can see, how the xml string is stored:</p>
  <pre class="brush:csharp;">private static string xmlText; //note the static keyword!
</pre>

  <p>What the...? Well, well, here's what happened: the author of this class coded in a small little optimization. He thought: "In this app, the configuration is only modified by members of the support staff and to do it, they have to shut down the system, so, there is no need to read the XML file every time an XmlConfiguration object is created. I can save some CPU cycles and I/O operations by reading it only once when the first object is created. Another created object will just use the same XML!". Good for him, not so good for you. Why? Because (unless your Statement is evaluated prior to being fulfilled), your custom xml string will never actually be used!</p>

  <h3>"Test-After" ends up as "Test-Never"</h3>

  <p>I'll ask this question again: ever had to write a requirement or design document for something that you already implemented? Was it fun? Was it valuable? Was it creative? No, I don't think so. The same is with our executable specification. After we write the code, we have little motivation to specify what's already written - some of the pieces of code "we can just see are correct", other pieces "we already saw working" when we copied our code over to our deployment machine and ran few sanity checks... The design is ready... Specification? Maybe next time...</p>

  <p>Another reason might be time pressure. Let's be honest - we're all in a hurry, we're all under pressure and when this pressure is too high, it triggers heroic behaviors in us, especially when there's a risk of not making it with the sprint commitment. Such heroic behavior usually goes by the following rules: drop all the "baggage", stop learning and experimenting, revert to all of the old "safe" behaviors and "save what we can!". If Specification is written at the end, it is often sacrificed on the altar of making it with the commitment, since the code is already written, "and it will be tested anyway by real tests" (box tests, smoke tests, sanity tests etc.). It is quite the contrary when starting with a Statement, where the Statement evaluating to false is <strong>a reason</strong> to write any code. Thus, if we want to write code, Specification become irremovable part of your development. By the way, I bet in big corporations no one sane ever thinks they can abandon checking in the code to source control, at the same time treating Specification as "an optional addition".</p><!-- TODO -->

  <h3>Not starting from specification leads to waste of time on making objects testable</h3>

  <p>It so happens, that I like watching and reading Uncle Bob. One day, I was listening to <a href="http://www.confreaks.com/videos/759-rubymidwest2011-keynote-architecture-the-lost-years" target="_blank">his keynote at Ruby Midwest 2011, called Architecture The Lost Years</a>. At the end, Robert made some digressions, one of them being about TDD. He said that writing unit tests after the code is not TDD. It is a waste of time.</p>

  <p>My initial thought was that the comment was only about missing all the benefits that starting with false Statement brings you: the ability to see the Statement fail, the ability to do a clean-sheet analysis etc., however, now I'm of opinion that there is more to it. It's something I got from Amir Kolsky and Scott Bain - in order to be able to write maintainable Specification for a piece of code, the code has to have a high level of a quality called <strong>testability</strong> (we'll talk about testability later on, don't worry - for now let's assume that the easier it is to write a Statement for a behavior of a class, the higher testability it has). That doesn't tell us much about where's the waste I mentioned, does it? To see it, let's see how dealing with testability looks like in Statement-first workflow (let's assume that we're creating new code, not adding stuff to dirty, ugly legacy code):</p>

  <ol>
    <li>Write false Statement (this step ensures that code has high testability)</li>

    <li>Write code to make the Statement true</li>
  </ol>

  <p>Now, how does it usually look like when we write the code first (extra steps marked with <strong>strong text</strong>):</p>

  <ol>
    <li>Write some production code (probably spans few classes until we're satisfied)</li>

    <li><strong>Start writing unit tests</strong></li>

    <li><strong>Notice that unit testing the whole set of classes is cumbersome and unsustainable and contains high redundancy.</strong></li>

    <li><strong>Restructure the code to be able to isolate objects and use mocks (this step ensures that code has high testability)</strong></li>

    <li>Write unit tests</li>
  </ol>

  <p>What's the equivalent of the marked steps in Statement-First approach? Nothing! Doing these things is a waste of time! Sadly, this is a waste I see done over and over again.</p>
</body>
</html>
